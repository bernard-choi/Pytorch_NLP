{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bo-LSTM(Attention) parameters\n",
    "embedding_dim = 2\n",
    "n_hidden = 5 # number of hidden units in one cell\n",
    "num_classes = 2 # 0 or 1\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 words sentences (=sequence_length is 3)\n",
    "sentences = [\"i love you\", \"he loves me\", 'shee likes baseball','I hate you','sorry for that','this is awful']\n",
    "labels = [1,1,1,0,0,0] # 1: 긍정 0: 부정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "vocab_size = len(word_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for sen in sentences:\n",
    "    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for out in labels:\n",
    "    targets.append(out) # To using Torch Softmax Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = Variable(torch.LongTensor(inputs))\n",
    "target_batch = Variable(torch.LongTensor(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8, 16],\n",
       "        [ 5, 15,  3],\n",
       "        [ 1, 14,  6],\n",
       "        [13,  9, 16],\n",
       "        [ 4, 10,  2],\n",
       "        [11,  0, 12]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention,self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm  = nn.LSTM(embedding_dim, n_hidden, bidirectional= True)\n",
    "        self.out = nn.Linear(n_hidden*2, num_classes)\n",
    "        \n",
    "    # lstm_output : [batch_size, n_step, n_hidden*num_directions(=2)], F matrix\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        \n",
    "        hidden = final_state.view(-1,n_hidden*2,1) # hidden:[batch_size, n_hidden*num_directions(=2), 1(=n_layer)]\n",
    "\n",
    "        attn_weights = torch.bmm(lstm_output,hidden).squeeze(2)      # attn_weights:[batch_size, len_seq]\n",
    "        soft_attn_weights = F.softmax(attn_weights,1)                # soft_attn_weights : [batch_size, len_seq, 1]\n",
    "        \n",
    "        context = torch.bmm(lstm_output.transpose(1,2), soft_attn_weights.unsqueeze(2)).squeeze(2) # lstm_output.transpose(1,2): [batch_size, n_hidden, len_seq]\n",
    "        return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "    \n",
    "    def forward(self, X):\n",
    "        input = self.embedding(X) # input :[batch_size, len_seq, n_hidden]\n",
    "        input = input.permute(1,0,2) # input : [len_seq, batch_size, n_hidden]\n",
    "        \n",
    "        hidden_state = Variable(torch.zeros(1*2, len(X), n_hidden)) # [num_layers(=1)*num_directions(=2),batch_size,n_hidden] 0으로 초기화\n",
    "        cell_state = Variable(torch.zeros(1*2,len(X), n_hidden))    # [num_layers(=1)*num_directions(=2), batch_size, n_hidden] 0으로 초기화\n",
    "        \n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        \n",
    "        output = output.permute(1,0,2) # output : [batch_size, len_seq, n_hidden*2]\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)\n",
    "        return self.out(attn_output), attention # model: [batch_size, num_classes], attention: [batch_size, n_step]\n",
    "    \n",
    "model = BiLSTM_Attention()\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1000 cost =  0.000084\n",
      "Epoch:  2000 cost =  0.000024\n",
      "Epoch:  3000 cost =  0.000010\n",
      "Epoch:  4000 cost =  0.000005\n",
      "Epoch:  5000 cost =  0.000003\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    output,attention = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch +1) % 1000 == 0:\n",
    "        print('Epoch: ', \"%04d\" % (epoch + 1), 'cost = ', '{:.6f}'.format(loss))\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_text = 'sorry hate you'\n",
    "tests = [np.asarray([word_dict[n] for n in test_text.split()])]\n",
    "test_batch = Variable(torch.LongTensor(tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.6350, -7.6986]], grad_fn=<AddmmBackward>),\n",
       " array([[0.65199715, 0.32035777, 0.02764505]], dtype=float32))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry hate you is Bad Mean...\n"
     ]
    }
   ],
   "source": [
    "predict,_ = model(test_batch)\n",
    "predict = predict.data.max(1,keepdim=True)[1]\n",
    "if predict[0][0] == 0:\n",
    "    print(test_text,\"is Bad Mean...\")\n",
    "else:\n",
    "    print(test_text,\"is Good Mean!!\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
