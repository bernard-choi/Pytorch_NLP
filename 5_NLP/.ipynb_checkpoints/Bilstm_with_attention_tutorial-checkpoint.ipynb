{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import data_utils\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "with open('word_dict.pickle', 'rb') as handle:\n",
    "    word_dict = pickle.load(handle)\n",
    "    \n",
    "train_x, train_y = build_word_dataset('train', word_dict,50) ##최대 50 token 나머지는 패딩\n",
    "test_x, test_y = build_word_dataset('test', word_dict,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bo-LSTM(Attention) parameters\n",
    "embedding_dim = 256\n",
    "n_hidden = 128 # number of hidden units in one cell\n",
    "num_classes = 2 # 0 or 1\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "seq_len = 50\n",
    "vocab_size = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention,self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm  = nn.LSTM(embedding_dim, n_hidden, bidirectional= True)\n",
    "        self.out = nn.Linear(n_hidden*2, num_classes)\n",
    "        \n",
    "    # lstm_output : [batch_size, n_step, n_hidden*num_directions(=2)], F matrix\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        \n",
    "        hidden = final_state.view(-1,n_hidden*2,1) # hidden:[batch_size, n_hidden*num_directions(=2), 1(=n_layer)]\n",
    "\n",
    "        attn_weights = torch.bmm(lstm_output,hidden).squeeze(2)      # attn_weights:[batch_size, len_seq]\n",
    "        soft_attn_weights = F.softmax(attn_weights,1)                # soft_attn_weights : [batch_size, len_seq, 1]\n",
    "        \n",
    "        context = torch.bmm(lstm_output.transpose(1,2), soft_attn_weights.unsqueeze(2)).squeeze(2) # lstm_output.transpose(1,2): [batch_size, n_hidden, len_seq]\n",
    "        return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "    \n",
    "    def forward(self, X):\n",
    "        input = self.embedding(X) # input :[batch_size, len_seq, n_hidden]\n",
    "        input = input.permute(1,0,2) # input : [len_seq, batch_size, n_hidden]\n",
    "        \n",
    "        hidden_state = Variable(torch.zeros(1*2, len(X), n_hidden)) # [num_layers(=1)*num_directions(=2),batch_size,n_hidden] 0으로 초기화\n",
    "        cell_state = Variable(torch.zeros(1*2,len(X), n_hidden))    # [num_layers(=1)*num_directions(=2), batch_size, n_hidden] 0으로 초기화\n",
    "        \n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        \n",
    "        output = output.permute(1,0,2) # output : [batch_size, len_seq, n_hidden*2]\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)\n",
    "        return self.out(attn_output), attention # model: [batch_size, num_classes], attention: [batch_size, n_step]\n",
    "    \n",
    "model = BiLSTM_Attention()\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4268805567920208 Step: 100\n",
      "Loss: 0.5120247811079025 Step: 200\n",
      "Loss: 0.48895525962114333 Step: 300\n",
      "Loss: 0.46548069149255755 Step: 400\n",
      "Loss: 0.4339281308650971 Step: 500\n",
      "Loss: 0.4307232415676117 Step: 600\n",
      "Loss: 0.4171468159556389 Step: 700\n",
      "Loss: 0.42360946282744405 Step: 800\n",
      "Loss: 0.42844311326742174 Step: 900\n",
      "Loss: 0.4167817008495331 Step: 1000\n",
      "Loss: 0.4175323759019375 Step: 1100\n",
      "Loss: 0.4243927384912968 Step: 1200\n",
      "Loss: 0.42445930540561677 Step: 1300\n",
      "Loss: 0.413636591732502 Step: 1400\n",
      "Loss: 0.41041492521762846 Step: 1500\n",
      "Loss: 0.4123778466880321 Step: 1600\n",
      "Loss: 0.41610816329717637 Step: 1700\n",
      "Loss: 0.4009539279341698 Step: 1800\n",
      "Loss: 0.40881083846092225 Step: 1900\n",
      "Loss: 0.39300848796963694 Step: 2000\n",
      "Loss: 0.4137266191840172 Step: 2100\n",
      "Loss: 0.41189706161618234 Step: 2200\n",
      "Loss: 0.38762944623827933 Step: 2300\n",
      "Loss: 0.3676491089165211 Step: 2400\n",
      "Loss: 0.352407388985157 Step: 2500\n",
      "Loss: 0.36746615439653396 Step: 2600\n",
      "Loss: 0.3631678073108196 Step: 2700\n",
      "Loss: 0.35467688605189324 Step: 2800\n",
      "Loss: 0.36801707863807676 Step: 2900\n",
      "Loss: 0.35201705917716025 Step: 3000\n",
      "Loss: 0.3568370074033737 Step: 3100\n",
      "Loss: 0.35690173029899597 Step: 3200\n",
      "Loss: 0.3725225377082825 Step: 3300\n",
      "Loss: 0.35905649065971373 Step: 3400\n",
      "Loss: 0.36081409513950347 Step: 3500\n",
      "Loss: 0.37053719729185103 Step: 3600\n",
      "Loss: 0.3680658732354641 Step: 3700\n",
      "Loss: 0.34814617469906806 Step: 3800\n",
      "Loss: 0.356524221599102 Step: 3900\n",
      "Loss: 0.3864237178862095 Step: 4000\n",
      "Loss: 0.3594423046708107 Step: 4100\n",
      "Loss: 0.35811297699809075 Step: 4200\n",
      "Loss: 0.3626003348827362 Step: 4300\n",
      "Loss: 0.36974130585789683 Step: 4400\n",
      "Loss: 0.37272905245423316 Step: 4500\n",
      "Loss: 0.35188484281301496 Step: 4600\n",
      "Loss: 0.3411182236671448 Step: 4700\n",
      "Loss: 0.31842884987592696 Step: 4800\n",
      "Loss: 0.3345719476044178 Step: 4900\n",
      "Loss: 0.33162629246711733 Step: 5000\n",
      "Loss: 0.3273679494857788 Step: 5100\n",
      "Loss: 0.3282931725680828 Step: 5200\n",
      "Loss: 0.33527853146195413 Step: 5300\n",
      "Loss: 0.3173082934319973 Step: 5400\n",
      "Loss: 0.33347890540957453 Step: 5500\n",
      "Loss: 0.35768011823296547 Step: 5600\n",
      "Loss: 0.3490892994403839 Step: 5700\n",
      "Loss: 0.3437774695456028 Step: 5800\n",
      "Loss: 0.3355799478292465 Step: 5900\n",
      "Loss: 0.3460599653422832 Step: 6000\n",
      "Loss: 0.3425353975594044 Step: 6100\n",
      "Loss: 0.34439857095479964 Step: 6200\n",
      "Loss: 0.34393661066889764 Step: 6300\n",
      "Loss: 0.36191869646310804 Step: 6400\n",
      "Loss: 0.3386260125041008 Step: 6500\n",
      "Loss: 0.35801925241947175 Step: 6600\n",
      "Loss: 0.3396830874681473 Step: 6700\n",
      "Loss: 0.37196302637457845 Step: 6800\n",
      "Loss: 0.35977158322930336 Step: 6900\n",
      "Loss: 0.3364000543951988 Step: 7000\n",
      "Loss: 0.3289075942337513 Step: 7100\n",
      "Loss: 0.31753527477383614 Step: 7200\n",
      "Loss: 0.3258934953808785 Step: 7300\n",
      "Loss: 0.31760362729430197 Step: 7400\n",
      "Loss: 0.3182546743750572 Step: 7500\n",
      "Loss: 0.32225760161876676 Step: 7600\n",
      "Loss: 0.3271052046120167 Step: 7700\n",
      "Loss: 0.3256055237352848 Step: 7800\n",
      "Loss: 0.3373974123597145 Step: 7900\n",
      "Loss: 0.34588805586099625 Step: 8000\n",
      "Loss: 0.3342282004654408 Step: 8100\n",
      "Loss: 0.3440610785782337 Step: 8200\n",
      "Loss: 0.32246781602501867 Step: 8300\n",
      "Loss: 0.3434982395172119 Step: 8400\n",
      "Loss: 0.32937360301613805 Step: 8500\n",
      "Loss: 0.33573887586593626 Step: 8600\n",
      "Loss: 0.3396041272580624 Step: 8700\n",
      "Loss: 0.3449870526790619 Step: 8800\n",
      "Loss: 0.34689616709947585 Step: 8900\n",
      "Loss: 0.3435123561322689 Step: 9000\n",
      "Loss: 0.32637458726763724 Step: 9100\n",
      "Loss: 0.34808926180005073 Step: 9200\n",
      "Loss: 0.3355190707743168 Step: 9300\n",
      "Loss: 0.3237604320049286 Step: 9400\n",
      "Loss: 0.3003693740069866 Step: 9500\n",
      "Loss: 0.32373641163110733 Step: 9600\n",
      "Loss: 0.33206631064414976 Step: 9700\n",
      "Loss: 0.3055266310274601 Step: 9800\n",
      "Loss: 0.30582253366708756 Step: 9900\n",
      "Loss: 0.3160663437843323 Step: 10000\n",
      "Loss: 0.308106005191803 Step: 10100\n",
      "Loss: 0.3199274508655071 Step: 10200\n",
      "Loss: 0.3326708287000656 Step: 10300\n",
      "Loss: 0.32286506675183774 Step: 10400\n",
      "Loss: 0.3393742328882217 Step: 10500\n",
      "Loss: 0.32440694242715834 Step: 10600\n",
      "Loss: 0.32928729832172393 Step: 10700\n",
      "Loss: 0.3258815474808216 Step: 10800\n",
      "Loss: 0.3184047792851925 Step: 10900\n",
      "Loss: 0.3323070211708546 Step: 11000\n",
      "Loss: 0.3412394918501377 Step: 11100\n",
      "Loss: 0.3203388075530529 Step: 11200\n",
      "Loss: 0.3270906385779381 Step: 11300\n",
      "Loss: 0.326889722943306 Step: 11400\n",
      "Loss: 0.3515599544346333 Step: 11500\n",
      "Loss: 0.33727518528699874 Step: 11600\n",
      "Loss: 0.32777357414364816 Step: 11700\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_batches = batch_iter(train_x,train_y,batch_size,num_epochs)\n",
    "num_batches_per_epochs = (len(train_x)-1) // batch_size +1\n",
    "step = 0\n",
    "running_loss = 0\n",
    "    \n",
    "for x_batch,y_batch in train_batches:\n",
    "        \n",
    "    #FeedForward\n",
    "        \n",
    "    step += 1\n",
    "    x_batch = Variable(torch.LongTensor(x_batch))\n",
    "    y_batch = Variable(torch.LongTensor(y_batch))\n",
    "    output, attention = model(x_batch)\n",
    "    loss = criterion(output,y_batch)\n",
    "        \n",
    "    #Backprop and update weight\n",
    "       \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    #loss 출력\n",
    "    running_loss += loss.item()\n",
    "    if (step % 100 == 0):\n",
    "        print(\"Loss: {}\".format(running_loss/100),'Step: {}'.format(step))\n",
    "        running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
