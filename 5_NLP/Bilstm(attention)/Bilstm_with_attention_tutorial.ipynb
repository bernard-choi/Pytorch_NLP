{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import data_utils\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "with open('word_dict.pickle', 'rb') as handle:\n",
    "    word_dict = pickle.load(handle)\n",
    "    \n",
    "train_x, train_y = build_word_dataset('train', word_dict,50) ##최대 50 token 나머지는 패딩\n",
    "test_x, test_y = build_word_dataset('test', word_dict,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bo-LSTM(Attention) parameters\n",
    "embedding_dim = 256\n",
    "n_hidden = 128 # number of hidden units in one cell\n",
    "num_classes = 2 # 0 or 1\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "seq_len = 50\n",
    "vocab_size = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention,self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm  = nn.LSTM(embedding_dim, n_hidden, bidirectional= True)\n",
    "        self.out = nn.Linear(n_hidden*2, num_classes)\n",
    "        \n",
    "    # lstm_output : [batch_size, n_step, n_hidden*num_directions(=2)], F matrix\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        \n",
    "        hidden = final_state.view(-1,n_hidden*2,1) # hidden:[batch_size, n_hidden*num_directions(=2), 1(=n_layer)]\n",
    "\n",
    "        attn_weights = torch.bmm(lstm_output,hidden).squeeze(2)      # attn_weights:[batch_size, len_seq]\n",
    "        soft_attn_weights = F.softmax(attn_weights,1)                # soft_attn_weights : [batch_size, len_seq, 1]\n",
    "        \n",
    "        context = torch.bmm(lstm_output.transpose(1,2), soft_attn_weights.unsqueeze(2)).squeeze(2) # lstm_output.transpose(1,2): [batch_size, n_hidden, len_seq]\n",
    "        return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "    \n",
    "    def forward(self, X):\n",
    "        input = self.embedding(X) # input :[batch_size, len_seq, n_hidden]\n",
    "        input = input.permute(1,0,2) # input : [len_seq, batch_size, n_hidden]\n",
    "        \n",
    "        hidden_state = Variable(torch.zeros(1*2, len(X), n_hidden)) # [num_layers(=1)*num_directions(=2),batch_size,n_hidden] 0으로 초기화\n",
    "        cell_state = Variable(torch.zeros(1*2,len(X), n_hidden))    # [num_layers(=1)*num_directions(=2), batch_size, n_hidden] 0으로 초기화\n",
    "        \n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        \n",
    "        output = output.permute(1,0,2) # output : [batch_size, len_seq, n_hidden*2]\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)\n",
    "        return self.out(attn_output), attention # model: [batch_size, num_classes], attention: [batch_size, n_step]\n",
    "    \n",
    "model = BiLSTM_Attention()\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4268805567920208 Step: 100\n",
      "Loss: 0.5120247811079025 Step: 200\n",
      "Loss: 0.48895525962114333 Step: 300\n",
      "Loss: 0.46548069149255755 Step: 400\n",
      "Loss: 0.4339281308650971 Step: 500\n",
      "Loss: 0.4307232415676117 Step: 600\n",
      "Loss: 0.4171468159556389 Step: 700\n",
      "Loss: 0.42360946282744405 Step: 800\n",
      "Loss: 0.42844311326742174 Step: 900\n",
      "Loss: 0.4167817008495331 Step: 1000\n",
      "Loss: 0.4175323759019375 Step: 1100\n",
      "Loss: 0.4243927384912968 Step: 1200\n",
      "Loss: 0.42445930540561677 Step: 1300\n",
      "Loss: 0.413636591732502 Step: 1400\n",
      "Loss: 0.41041492521762846 Step: 1500\n",
      "Loss: 0.4123778466880321 Step: 1600\n",
      "Loss: 0.41610816329717637 Step: 1700\n",
      "Loss: 0.4009539279341698 Step: 1800\n",
      "Loss: 0.40881083846092225 Step: 1900\n",
      "Loss: 0.39300848796963694 Step: 2000\n",
      "Loss: 0.4137266191840172 Step: 2100\n",
      "Loss: 0.41189706161618234 Step: 2200\n",
      "Loss: 0.38762944623827933 Step: 2300\n",
      "Loss: 0.3676491089165211 Step: 2400\n",
      "Loss: 0.352407388985157 Step: 2500\n",
      "Loss: 0.36746615439653396 Step: 2600\n",
      "Loss: 0.3631678073108196 Step: 2700\n",
      "Loss: 0.35467688605189324 Step: 2800\n",
      "Loss: 0.36801707863807676 Step: 2900\n",
      "Loss: 0.35201705917716025 Step: 3000\n",
      "Loss: 0.3568370074033737 Step: 3100\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_batches = batch_iter(train_x,train_y,batch_size,num_epochs)\n",
    "num_batches_per_epochs = (len(train_x)-1) // batch_size +1\n",
    "step = 0\n",
    "running_loss = 0\n",
    "    \n",
    "for x_batch,y_batch in train_batches:\n",
    "        \n",
    "    #FeedForward\n",
    "        \n",
    "    step += 1\n",
    "    x_batch = Variable(torch.LongTensor(x_batch))\n",
    "    y_batch = Variable(torch.LongTensor(y_batch))\n",
    "    output, attention = model(x_batch)\n",
    "    loss = criterion(output,y_batch)\n",
    "        \n",
    "    #Backprop and update weight\n",
    "       \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    #loss 출력\n",
    "    running_loss += loss.item()\n",
    "    if (step % 100 == 0):\n",
    "        print(\"Loss: {}\".format(running_loss/100),'Step: {}'.format(step))\n",
    "        running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
