{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'E',\n",
       " 'P',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 0,\n",
       " 'E': 1,\n",
       " 'P': 2,\n",
       " 'a': 3,\n",
       " 'b': 4,\n",
       " 'c': 5,\n",
       " 'd': 6,\n",
       " 'e': 7,\n",
       " 'f': 8,\n",
       " 'g': 9,\n",
       " 'h': 10,\n",
       " 'i': 11,\n",
       " 'j': 12,\n",
       " 'k': 13,\n",
       " 'l': 14,\n",
       " 'm': 15,\n",
       " 'n': 16,\n",
       " 'o': 17,\n",
       " 'p': 18,\n",
       " 'q': 19,\n",
       " 'r': 20,\n",
       " 's': 21,\n",
       " 't': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'w': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['man', 'women'],\n",
       " ['black', 'white'],\n",
       " ['king', 'queen'],\n",
       " ['girl', 'boy'],\n",
       " ['up', 'down'],\n",
       " ['high', 'low']]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2seq parameter\n",
    "n_step = 5\n",
    "n_hidden = 128\n",
    "n_class = len(num_dic)\n",
    "batch_size = len(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        output_batch.append(np.eye(n_class)[output])\n",
    "        target_batch.append(target) # not one-hot\n",
    "\n",
    "    # make tensor\n",
    "    return Variable(torch.Tensor(input_batch)), Variable(torch.Tensor(output_batch)), Variable(torch.LongTensor(target_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.enc_cell = nn.RNN(input_size = n_class, hidden_size = n_hidden, dropout = 0.5)\n",
    "        self.dec_cell = nn.RNN(input_size = n_class, hidden_size = n_hidden, dropout = 0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "        \n",
    "    def forward(self,enc_input,enc_hidden,dec_input):\n",
    "        enc_input = enc_input.transpose(0,1) # enc_input : [seq_len, batch_size, n_class]\n",
    "        dec_input = dec_input.transpose(0,1) # dec_input : [seq_len+1, batch_size, n_class]\n",
    "        \n",
    "        # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        \n",
    "        # outputs : [seq_len+1, batchsize, n_hidden]\n",
    "        outputs,_ = self.dec_cell(dec_input,enc_states)\n",
    "        \n",
    "        model = self.fc(outputs) # model : [seq_len+1, batch_size. n_class]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.003328\n",
      "Epoch: 2000 cost = 0.000919\n",
      "Epoch: 3000 cost = 0.000395\n",
      "Epoch: 4000 cost = 0.000198\n",
      "Epoch: 5000 cost = 0.000107\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5000):\n",
    "    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "    hidden = Variable(torch.zeros(1,batch_size,n_hidden))\n",
    "    # input_batch : [batch_size, seq_len+1, n_class]\n",
    "    # output_batch : [batch_size, seq_len+1,n_class]\n",
    "    # target_batch : [batch_size, seq_len+1], not one-hot\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch, hidden, output_batch)\n",
    "    #output : [seq_len+1, batch_size, n_class]\n",
    "    output = output.transpose(0,1) # [batch_size, seq_len+1,n_class(=29)]\n",
    "    loss = 0\n",
    "    for i in range(0,len(target_batch)):\n",
    "        # output[i] : [seq_len+1,n_class], target_batch : [seq_len+1] \n",
    "        loss += criterion(output[i], target_batch[i])\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.000577\n",
      "Epoch: 2000 cost = 0.000161\n",
      "Epoch: 3000 cost = 0.000069\n",
      "Epoch: 4000 cost = 0.000035\n",
      "Epoch: 5000 cost = 0.000019\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5000):\n",
    "    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "    hidden = Variable(torch.zeros(1,batch_size,n_hidden))\n",
    "    # input_batch : [batch_size, seq_len+1, n_class]\n",
    "    # output_batch : [batch_size, seq_len+1,n_class]\n",
    "    # target_batch : [batch_size, seq_len+1], not one-hot\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch, hidden, output_batch)\n",
    "    #output : [seq_len+1, batch_size, n_class]\n",
    "    output = output.transpose(0,1) # [batch_size, seq_len+1,n_class(=29)]\n",
    "    loss = 0\n",
    "\n",
    "    output = output.contiguous().view(-1,n_class) # output : [batch_size*seq_len+1, n_class(=29)]\n",
    "    target = target_batch.view(-1)   # target : [batch_size*seq_len+1]\n",
    "    loss += criterion(output, target)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
