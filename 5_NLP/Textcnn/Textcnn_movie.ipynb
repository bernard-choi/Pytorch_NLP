{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from data_utils import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_dict.pickle', 'rb') as handle:\n",
    "    word_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = build_word_dataset('train', word_dict,50) ##최대 50 token 나머지는 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = build_word_dataset('test', word_dict,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e2f7c4aed698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49898"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256 \n",
    "sequence_length = 50\n",
    "num_classes = 2\n",
    "filter_sizes = [2,3,4] # n-gram 다양한 filtersize로 filter를 적용하겠다. \n",
    "num_filters = 10\n",
    "vocab_size = len(word_dict)\n",
    "num_epochs = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN,self).__init__()\n",
    "        \n",
    "        self.num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.W = nn.Parameter(torch.empty(vocab_size,embedding_size).uniform_(-1,1)).type(dtype)\n",
    "        self.Weight = nn.Parameter(torch.empty(self.num_filters_total,num_classes).uniform_(-1,1)).type(dtype)\n",
    "        self.Bias = nn.Parameter(0.1 * torch.ones([num_classes])).type(dtype)\n",
    "\n",
    "        \n",
    "    def forward(self,X):\n",
    "        embedded_chars = self.W[X] # [batch_size, sequence_length, embedding_size]\n",
    "        embedded_chars = embedded_chars.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n",
    "        \n",
    "        pooled_outputs =[]\n",
    "        for filter_size in filter_sizes:\n",
    "            # conv : [input_channel(=1), output_channel(=3), (filter_height, filter_width), bias_option]\n",
    "            conv = nn.Conv2d(1,num_filters,(filter_size, embedding_size), bias = True)(embedded_chars) ## conv2d를 embedded_chars에 적용\n",
    "            h = F.relu(conv)\n",
    "            # mp : ((filter_height, filter_width))\n",
    "            mp = nn.MaxPool2d((sequence_length-filter_size+1,1))\n",
    "            # pooled : [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3)]\n",
    "            pooled = mp(h).permute(0,3,2,1)\n",
    "            pooled_outputs.append(pooled)\n",
    "            \n",
    "        h_pool = torch.cat(pooled_outputs,len(filter_sizes)) ## channel을 늘인거 [batch_size(=6), output_height(=1), output_width(=1),output_channel(=3)*3]\n",
    "        h_pool_flat = torch.reshape(h_pool, [-1,self.num_filters_total]) # [batch_size(=6), output_height * output_width * (output_channel *3)] 여기서 output_channel *3이 결국 num_filters_total이 된다\n",
    "        model = torch.mm(h_pool_flat,self.Weight) + self.Bias # [batch_size, num_classes]    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = TextCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5632700380682946 Epoch: 10 Step: 100\n",
      "Loss: 0.5584706944227219 Epoch: 10 Step: 200\n",
      "Loss: 0.5652339935302735 Epoch: 10 Step: 300\n",
      "Loss: 0.5741479927301407 Epoch: 10 Step: 400\n",
      "Loss: 0.5643043798208237 Epoch: 10 Step: 500\n",
      "Loss: 0.5453199142217636 Epoch: 10 Step: 600\n",
      "Loss: 0.5639192405343055 Epoch: 10 Step: 700\n",
      "Loss: 0.5639493584632873 Epoch: 10 Step: 800\n",
      "Loss: 0.5541241294145585 Epoch: 10 Step: 900\n",
      "Loss: 0.5719846737384796 Epoch: 10 Step: 1000\n",
      "Loss: 0.5451348197460174 Epoch: 10 Step: 1100\n",
      "Loss: 0.5486654365062713 Epoch: 10 Step: 1200\n",
      "Loss: 0.55373967140913 Epoch: 10 Step: 1300\n",
      "Loss: 0.5629369115829468 Epoch: 10 Step: 1400\n",
      "Loss: 0.5485244327783585 Epoch: 10 Step: 1500\n",
      "Loss: 0.5639682605862617 Epoch: 10 Step: 1600\n",
      "Loss: 0.5548910310864449 Epoch: 10 Step: 1700\n",
      "Loss: 0.5642914342880249 Epoch: 10 Step: 1800\n",
      "Loss: 0.5571119791269302 Epoch: 10 Step: 1900\n",
      "Loss: 0.55411581158638 Epoch: 10 Step: 2000\n",
      "Loss: 0.5721792334318161 Epoch: 10 Step: 2100\n",
      "Loss: 0.5631011876463891 Epoch: 10 Step: 2200\n",
      "Loss: 0.5594449120759964 Epoch: 10 Step: 2300\n",
      "Loss: 0.5503845435380935 Epoch: 10 Step: 2400\n",
      "Loss: 0.548414671421051 Epoch: 10 Step: 2500\n",
      "Loss: 0.5525484323501587 Epoch: 10 Step: 2600\n",
      "Loss: 0.5599293476343155 Epoch: 10 Step: 2700\n",
      "Loss: 0.5560577630996704 Epoch: 10 Step: 2800\n",
      "Loss: 0.550967446565628 Epoch: 10 Step: 2900\n",
      "Loss: 0.5573570850491524 Epoch: 10 Step: 3000\n",
      "Loss: 0.5579461491107941 Epoch: 10 Step: 3100\n",
      "Loss: 0.5498338127136231 Epoch: 10 Step: 3200\n",
      "Loss: 0.5739959627389908 Epoch: 10 Step: 3300\n",
      "Loss: 0.5462872073054313 Epoch: 10 Step: 3400\n",
      "Loss: 0.5460770604014397 Epoch: 10 Step: 3500\n",
      "Loss: 0.545001328587532 Epoch: 10 Step: 3600\n",
      "Loss: 0.5572014319896698 Epoch: 10 Step: 3700\n",
      "Loss: 0.5523733079433442 Epoch: 10 Step: 3800\n",
      "Loss: 0.5515808144211769 Epoch: 10 Step: 3900\n",
      "Loss: 0.559043163061142 Epoch: 10 Step: 4000\n",
      "Loss: 0.5551356893777847 Epoch: 10 Step: 4100\n",
      "Loss: 0.5613775151968002 Epoch: 10 Step: 4200\n",
      "Loss: 0.5461664965748787 Epoch: 10 Step: 4300\n",
      "Loss: 0.5610634678602219 Epoch: 10 Step: 4400\n",
      "Loss: 0.5595857658982277 Epoch: 10 Step: 4500\n",
      "Loss: 0.5536920130252838 Epoch: 10 Step: 4600\n",
      "Loss: 0.5541403785347938 Epoch: 10 Step: 4700\n",
      "Loss: 0.5461751446127892 Epoch: 10 Step: 4800\n",
      "Loss: 0.5414108622074127 Epoch: 10 Step: 4900\n",
      "Loss: 0.5579728826880455 Epoch: 10 Step: 5000\n",
      "Loss: 0.5602683389186859 Epoch: 10 Step: 5100\n",
      "Loss: 0.5577359771728516 Epoch: 10 Step: 5200\n",
      "Loss: 0.5403265807032586 Epoch: 10 Step: 5300\n",
      "Loss: 0.5561911389231682 Epoch: 10 Step: 5400\n",
      "Loss: 0.5549639078974724 Epoch: 10 Step: 5500\n",
      "Loss: 0.5527405750751495 Epoch: 10 Step: 5600\n",
      "Loss: 0.5649836629629135 Epoch: 10 Step: 5700\n",
      "Loss: 0.536608766913414 Epoch: 10 Step: 5800\n",
      "Loss: 0.5410100391507149 Epoch: 10 Step: 5900\n",
      "Loss: 0.552071473300457 Epoch: 10 Step: 6000\n",
      "Loss: 0.551738269329071 Epoch: 10 Step: 6100\n",
      "Loss: 0.5518744906783104 Epoch: 10 Step: 6200\n",
      "Loss: 0.5537680333852768 Epoch: 10 Step: 6300\n",
      "Loss: 0.5497403049468994 Epoch: 10 Step: 6400\n",
      "Loss: 0.5578073307871818 Epoch: 10 Step: 6500\n",
      "Loss: 0.5495173993706703 Epoch: 10 Step: 6600\n",
      "Loss: 0.549847811460495 Epoch: 10 Step: 6700\n",
      "Loss: 0.5667272874712944 Epoch: 10 Step: 6800\n",
      "Loss: 0.5502860862016677 Epoch: 10 Step: 6900\n",
      "Loss: 0.5522490614652633 Epoch: 10 Step: 7000\n",
      "Loss: 0.5485283115506172 Epoch: 10 Step: 7100\n",
      "Loss: 0.5463022822141648 Epoch: 10 Step: 7200\n",
      "Loss: 0.5451221334934234 Epoch: 10 Step: 7300\n",
      "Loss: 0.5610752525925636 Epoch: 10 Step: 7400\n",
      "Loss: 0.5473641300201416 Epoch: 10 Step: 7500\n",
      "Loss: 0.5408762219548225 Epoch: 10 Step: 7600\n",
      "Loss: 0.5526340749859809 Epoch: 10 Step: 7700\n",
      "Loss: 0.5527086663246155 Epoch: 10 Step: 7800\n",
      "Loss: 0.5426143553853034 Epoch: 10 Step: 7900\n",
      "Loss: 0.568796019256115 Epoch: 10 Step: 8000\n",
      "Loss: 0.5401536282896996 Epoch: 10 Step: 8100\n",
      "Loss: 0.5358380708098411 Epoch: 10 Step: 8200\n",
      "Loss: 0.5370244565606117 Epoch: 10 Step: 8300\n",
      "Loss: 0.5571693933010101 Epoch: 10 Step: 8400\n",
      "Loss: 0.5443821826577186 Epoch: 10 Step: 8500\n",
      "Loss: 0.5486184453964233 Epoch: 10 Step: 8600\n",
      "Loss: 0.5556023606657982 Epoch: 10 Step: 8700\n",
      "Loss: 0.5477359667420387 Epoch: 10 Step: 8800\n",
      "Loss: 0.5576155251264572 Epoch: 10 Step: 8900\n",
      "Loss: 0.5458123347163201 Epoch: 10 Step: 9000\n",
      "Loss: 0.5580206903815269 Epoch: 10 Step: 9100\n",
      "Loss: 0.5521285536885262 Epoch: 10 Step: 9200\n",
      "Loss: 0.5452498358488083 Epoch: 10 Step: 9300\n",
      "Loss: 0.5515252801775933 Epoch: 10 Step: 9400\n",
      "Loss: 0.5400976267457008 Epoch: 10 Step: 9500\n",
      "Loss: 0.5414016935229301 Epoch: 10 Step: 9600\n",
      "Loss: 0.5530049368739128 Epoch: 10 Step: 9700\n",
      "Loss: 0.5548457619547844 Epoch: 10 Step: 9800\n",
      "Loss: 0.5474241584539413 Epoch: 10 Step: 9900\n",
      "Loss: 0.5415671467781067 Epoch: 10 Step: 10000\n",
      "Loss: 0.5503968042135239 Epoch: 10 Step: 10100\n",
      "Loss: 0.5487737545371055 Epoch: 10 Step: 10200\n",
      "Loss: 0.5605423414707184 Epoch: 10 Step: 10300\n",
      "Loss: 0.5467581185698509 Epoch: 10 Step: 10400\n",
      "Loss: 0.5347702869772911 Epoch: 10 Step: 10500\n",
      "Loss: 0.5372486129403115 Epoch: 10 Step: 10600\n",
      "Loss: 0.5431614562869072 Epoch: 10 Step: 10700\n",
      "Loss: 0.553250253200531 Epoch: 10 Step: 10800\n",
      "Loss: 0.536987498998642 Epoch: 10 Step: 10900\n",
      "Loss: 0.5497214883565903 Epoch: 10 Step: 11000\n",
      "Loss: 0.5433724075555801 Epoch: 10 Step: 11100\n",
      "Loss: 0.5577909508347512 Epoch: 10 Step: 11200\n",
      "Loss: 0.5384209981560707 Epoch: 10 Step: 11300\n",
      "Loss: 0.5483804923295975 Epoch: 10 Step: 11400\n",
      "Loss: 0.5601169627904892 Epoch: 10 Step: 11500\n",
      "Loss: 0.5445256391167641 Epoch: 10 Step: 11600\n",
      "Loss: 0.5453607267141343 Epoch: 10 Step: 11700\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "\n",
    "train_batches = batch_iter(train_x,train_y,batch_size,num_epochs)\n",
    "num_batches_per_epochs = (len(train_x)-1) // batch_size +1\n",
    "step = 0\n",
    "running_loss = 0\n",
    "    \n",
    "for x_batch,y_batch in train_batches:\n",
    "        \n",
    "    #FeedForward\n",
    "        \n",
    "    step += 1\n",
    "    x_batch = Variable(torch.LongTensor(x_batch))\n",
    "    y_batch = Variable(torch.LongTensor(y_batch))\n",
    "    output = model(x_batch)\n",
    "    loss = criterion(output,y_batch)\n",
    "        \n",
    "    #Backprop and update weight\n",
    "       \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    #loss 출력\n",
    "    running_loss += loss.item()\n",
    "    if (step % 100 == 0):\n",
    "        print(\"Loss: {}\".format(running_loss/100),\"Epoch: {}\".format(epoch+1),'Step: {}'.format(step))\n",
    "        running_loss = 0\n",
    "            \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
